# -*- coding: utf-8 -*-
"""EfficientNetB7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nLJo4cTv7X6zEOjXL92_sGjbwUnE9thb
"""

! pip install -q kaggle
from google.colab import files
files.upload()

!nvidia-smi

!gdown --id 1gR5-Tzr-HZicaORJEHg6uVOOcPy0YY49
!unzip rooms_dataset.zip

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import EfficientNetB7
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
import seaborn as sns
import os
import cv2
import random

# Loading the dataset

# Dataset path
dataset_path = "/content/rooms_dataset"

fish_classes = os.listdir(dataset_path) 
print(fish_classes)

# Creating dataframe of file path and class
df = pd.DataFrame(columns=['file_path', 'class'])
for fish_class in fish_classes:
    fish_class_path = os.path.join(dataset_path, fish_class)
    fish_class_files = os.listdir(fish_class_path)
    for fish_class_file in fish_class_files:
        fish_class_file_path = os.path.join(fish_class_path, fish_class_file)
        df = df.append({'file_path': fish_class_file_path, 'class': fish_class}, ignore_index=True)
    
df.head()

# Shuffling the dataset and resetting the index
df = df.sample(frac=1).reset_index(drop=True)
df.head()

# Diplaying 20 pictures with their class
plt.figure(figsize=(20, 20))
for i in range(20):
    plt.subplot(5, 4, i+1)
    img = cv2.imread(df['file_path'][i])
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.imshow(img)
    plt.title(df['class'][i])
    plt.axis('off')
plt.show()

# Splitting the dataset into the Training set and Test set
X_train, X_test, y_train, y_test = train_test_split(df['file_path'], df['class'], test_size = 0.2, random_state = 0)

train_generator = tf.keras.preprocessing.image.ImageDataGenerator(
    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input,
    validation_split=0.2)

test_generator = tf.keras.preprocessing.image.ImageDataGenerator(
    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input)

train_images = train_generator.flow_from_dataframe(
    dataframe=df,
    x_col="file_path",
    y_col="class",
    target_size=(75, 75),
    batch_size=32,
    class_mode='categorical',
    subset='training')

validation_images = train_generator.flow_from_dataframe(
    dataframe=df,
    x_col="file_path",
    y_col="class",
    target_size=(75, 75),
    batch_size=32,
    class_mode='categorical',
    subset='validation')

test_images = test_generator.flow_from_dataframe(
    dataframe=df,
    x_col="file_path",
    y_col="class",
    target_size=(75, 75),
    batch_size=32,
    class_mode='categorical',
    shuffle=False)

def create_model(input_shape = (75, 75, 3)):

    inputs = tf.keras.layers.Input(shape=input_shape)
    base_model = EfficientNetB7(include_top=False, weights='imagenet', input_tensor=inputs, classes = 9)

    x = base_model(inputs)
    
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    x = tf.keras.layers.Dropout(0.2)(x)

    outputs = tf.keras.layers.Dense(9, activation='sigmoid')(x)
    model = tf.keras.Model(inputs, outputs)
    return model

model = create_model((75, 75, 3))
metrics = [tf.keras.metrics.CategoricalAccuracy(name='accuracy')]
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=metrics)
model.summary()

callbacks = [
    tf.keras.callbacks.ModelCheckpoint('model.h5', save_best_only=True, monitor='val_accuracy', mode='max'),
    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, mode='max'),
    ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=3, verbose=1, mode='max', min_delta=0.0001)]

# Training the model
history = model.fit(train_images, epochs=8, validation_data=validation_images, callbacks=callbacks)

# Visualizing the results test accuracy and test loss
test_loss, test_accuracy = model.evaluate(test_images)
print('Test accuracy :', test_accuracy)
print('Test loss :', test_loss)

# Plotting accuracy and loss
plt.figure(figsize=(15, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

# Classification report
y_pred = model.predict(test_images)
y_pred = np.argmax(y_pred, axis=1)
print('Classification Report')
print(classification_report(test_images.classes, y_pred, target_names=test_images.class_indices.keys()))

# Confusion matrix with heatmap
cm = confusion_matrix(test_images.classes, y_pred)
plt.figure(figsize=(10, 10))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()